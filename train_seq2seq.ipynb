{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the phylogenetic embeddings (seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries, set up global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from model.seq2seq import basic_seq2seq\n",
    "import numpy as np\n",
    "from utils.text_processing import load_dict_from_vocab_file\n",
    "import os\n",
    "\n",
    "vocab_file = './data/character_inventory_unk.txt'\n",
    "traindb_file = './data/training.npz'\n",
    "testdb_file = './data/testing.npz'\n",
    "checkpoint_dir = './tfmodel/gru_enc'\n",
    "checkpoint_file = checkpoint_dir + '/model_%d.tfmodel'\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "log_dir = './tb'\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model and training constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "l2reg = 0.01\n",
    "keep_prob=1.0\n",
    "batch_size_val = 64\n",
    "vocab = load_dict_from_vocab_file(vocab_file)\n",
    "vocab_size = len(vocab)\n",
    "lstm_dim = 500\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define placeholders (model inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"placeholders\"):\n",
    "    encoder_in = tf.placeholder(tf.int32, [None, None])\n",
    "    encoder_lens = tf.placeholder(tf.int32, [None])\n",
    "    batch_size = tf.placeholder(tf.int32)\n",
    "    \n",
    "    decoder_in = tf.placeholder(tf.int32, [None, None])\n",
    "    decoder_lens = tf.placeholder(tf.int32, [None])\n",
    "    labels = tf.placeholder(tf.int32, [None, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the seq2seq, get the relevant tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"model\"):\n",
    "    logits, _ = basic_seq2seq(encoder_in, encoder_lens, decoder_in, decoder_lens,\n",
    "                                          vocab_size=vocab_size, batch_size=batch_size, lstm_type=\"gru\",\n",
    "                                          lstm_dim=lstm_dim, keep_prob=keep_prob, max_iterations=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss (crossent and L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    #labels_flat = tf.reshape(labels, [-1])\n",
    "    #logits = tf.reshape(logits, [-1, vocab_size])\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits)\n",
    "    \n",
    "    train_loss = (tf.reduce_sum(crossent) / tf.cast(batch_size, tf.float32))\n",
    "    \n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    reg_loss = l2reg * sum(reg_losses)\n",
    "    tv = tf.trainable_variables()\n",
    "    reg_losses.extend([l2reg * tf.nn.l2_loss(v) for v in tv])\n",
    "\n",
    "    loss = train_loss + reg_loss\n",
    "    \n",
    "    with tf.name_scope(\"logging\"):\n",
    "        tf.summary.scalar(\"train_loss\", train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer (Adam, gradient clipping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name gru/rnn/gru_cell/gates/kernel:0_grads is illegal; using gru/rnn/gru_cell/gates/kernel_0_grads instead.\n",
      "INFO:tensorflow:Summary name gru/rnn/gru_cell/gates/bias:0_grads is illegal; using gru/rnn/gru_cell/gates/bias_0_grads instead.\n",
      "INFO:tensorflow:Summary name gru/rnn/gru_cell/candidate/kernel:0_grads is illegal; using gru/rnn/gru_cell/candidate/kernel_0_grads instead.\n",
      "INFO:tensorflow:Summary name gru/rnn/gru_cell/candidate/bias:0_grads is illegal; using gru/rnn/gru_cell/candidate/bias_0_grads instead.\n",
      "INFO:tensorflow:Summary name decoder/conditional_gru_cell/gates/kernel:0_grads is illegal; using decoder/conditional_gru_cell/gates/kernel_0_grads instead.\n",
      "INFO:tensorflow:Summary name decoder/conditional_gru_cell/candidate/kernel:0_grads is illegal; using decoder/conditional_gru_cell/candidate/kernel_0_grads instead.\n",
      "INFO:tensorflow:Summary name decoder/dense/kernel:0_grads is illegal; using decoder/dense/kernel_0_grads instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"optimizer\"):\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        gvs = optimizer.compute_gradients(loss)\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, 5.), var) for grad, var in gvs]\n",
    "        train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
    "    \n",
    "        with tf.name_scope(\"logging\"):\n",
    "            for grad, var in capped_gvs:\n",
    "                tf.summary.histogram(var.name + \"_grads\", grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish up with some logging hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name gru/rnn/gru_cell/gates/kernel:0 is illegal; using gru/rnn/gru_cell/gates/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name gru/rnn/gru_cell/gates/bias:0 is illegal; using gru/rnn/gru_cell/gates/bias_0 instead.\n",
      "INFO:tensorflow:Summary name gru/rnn/gru_cell/candidate/kernel:0 is illegal; using gru/rnn/gru_cell/candidate/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name gru/rnn/gru_cell/candidate/bias:0 is illegal; using gru/rnn/gru_cell/candidate/bias_0 instead.\n",
      "INFO:tensorflow:Summary name decoder/conditional_gru_cell/gates/kernel:0 is illegal; using decoder/conditional_gru_cell/gates/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name decoder/conditional_gru_cell/candidate/kernel:0 is illegal; using decoder/conditional_gru_cell/candidate/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name decoder/dense/kernel:0 is illegal; using decoder/dense/kernel_0 instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"logging\"):\n",
    "    valid_loss_ph = tf.placeholder(tf.float32, name=\"validation_loss\")\n",
    "    \n",
    "    tf.summary.scalar(\"Valid_loss\", valid_loss_ph)\n",
    "\n",
    "    #Add histograms for trainable variables\n",
    "    for v in tf.trainable_variables():\n",
    "        tf.summary.histogram(v.name, v)    \n",
    "\n",
    "    log_op = tf.summary.merge_all()\n",
    "\n",
    "writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop. On each logging interval, we calculate the validation loss and save the weights when the loss improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "180.69785\n",
      "139.92558\n",
      "136.77997\n",
      "136.04811\n",
      "132.77586\n",
      "131.74165\n",
      "123.625145\n",
      "119.47009\n",
      "111.52328\n",
      "106.8621\n",
      "94.97787\n",
      "88.87121\n",
      "82.81174\n",
      "75.829025\n",
      "72.38384\n",
      "68.40091\n",
      "67.44328\n",
      "61.35666\n",
      "57.48249\n",
      "57.08316\n",
      "53.066074\n",
      "52.698963\n",
      "50.001938\n",
      "49.268017\n",
      "48.29007\n",
      "45.932804\n",
      "45.646046\n",
      "45.63324\n",
      "43.621532\n",
      "43.078365\n",
      "41.092403\n",
      "40.24352\n",
      "38.852158\n",
      "38.228096\n",
      "37.29971\n",
      "37.111988\n",
      "36.687542\n",
      "35.030342\n"
     ]
    }
   ],
   "source": [
    "data = np.load(traindb_file)\n",
    "encoder_in_batch = data['enc_in']\n",
    "encoder_len_batch = data['enc_lens']\n",
    "decoder_in_batch = data['dec_in']\n",
    "decoder_len_batch = data['dec_lens']\n",
    "labels_batch = data['labels']\n",
    "\n",
    "valid_data = np.load(testdb_file)\n",
    "valid_encoder_in_batch = data['enc_in']\n",
    "valid_encoder_len_batch = data['enc_lens']\n",
    "valid_decoder_in_batch = data['dec_in']\n",
    "valid_decoder_len_batch = data['dec_lens']\n",
    "valid_labels_batch = data['labels']\n",
    "\n",
    "n_examples = labels_batch.shape[0]\n",
    "idx = np.arange(n_examples)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "i = 0\n",
    "k = 0\n",
    "min_loss_val = 10000.0\n",
    "while i < n_epochs:\n",
    "    print(\"EPOCH %d\"%i)\n",
    "    j = 0\n",
    "    np.random.shuffle(idx)\n",
    "    while j < n_examples:\n",
    "        curr = idx[j:j+batch_size_val]\n",
    "        batch_size_curr = len(curr)\n",
    "        if k % log_interval == 0:\n",
    "            #Get the validation loss\n",
    "            valid_loss_val, logits_val = sess.run([loss, logits], feed_dict={encoder_in: valid_encoder_in_batch,\n",
    "                                                       encoder_lens: valid_encoder_len_batch,\n",
    "                                                       decoder_in: valid_decoder_in_batch,\n",
    "                                                       decoder_lens: valid_decoder_len_batch,\n",
    "                                                       labels: valid_labels_batch,\n",
    "                                                       batch_size: valid_labels_batch.shape[0]})\n",
    "            \n",
    "            \n",
    "            summary, _, loss_val, logits_val = sess.run([log_op, train_op, loss, logits], feed_dict={encoder_in: encoder_in_batch[curr],\n",
    "                                                       encoder_lens: encoder_len_batch[curr],\n",
    "                                                       decoder_in: decoder_in_batch[curr],\n",
    "                                                       decoder_lens: decoder_len_batch[curr],\n",
    "                                                       labels: labels_batch[curr],\n",
    "                                                       batch_size: batch_size_curr,\n",
    "                                                       valid_loss_ph: valid_loss_val})\n",
    "            writer.add_summary(summary, k)\n",
    "            \n",
    "            if valid_loss_val < min_loss_val:\n",
    "                print(valid_loss_val)\n",
    "                min_loss_val = valid_loss_val\n",
    "                saver.save(sess, checkpoint_file % k)\n",
    "        else:\n",
    "            _, loss_val, logits_val = sess.run([train_op, loss, logits], feed_dict={encoder_in: encoder_in_batch[curr],\n",
    "                                                       encoder_lens: encoder_len_batch[curr],\n",
    "                                                       decoder_in: decoder_in_batch[curr],\n",
    "                                                       decoder_lens: decoder_len_batch[curr],\n",
    "                                                       labels: labels_batch[curr],\n",
    "                                                       batch_size: batch_size_curr})\n",
    "        \n",
    "        \n",
    "        j += batch_size_val\n",
    "        k += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phylogenetic-autoencoder",
   "language": "python",
   "name": "phylogenetic-autoencoder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
