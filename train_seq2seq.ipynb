{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the phylogenetic embeddings (seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries, set up global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from model.seq2seq import basic_seq2seq\n",
    "import numpy as np\n",
    "from utils.text_processing import load_dict_from_vocab_file\n",
    "\n",
    "vocab_file = './data/character_inventory_unk.txt'\n",
    "traindb_file = './data/training.npz'\n",
    "testdb_file = './data/testing.npz'\n",
    "checkpoint_file = './tfmodel/gru_enc/model_%d.tfmodel'\n",
    "log_dir = './tb'\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model and training constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "l2reg = 0.01\n",
    "keep_prob=1.0\n",
    "batch_size_val = 64\n",
    "vocab = load_dict_from_vocab_file(vocab_file)\n",
    "vocab_size = len(vocab)\n",
    "lstm_dim = 500\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define placeholders (model inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"placeholders\"):\n",
    "    encoder_in = tf.placeholder(tf.int32, [None, None])\n",
    "    encoder_lens = tf.placeholder(tf.int32, [None])\n",
    "    batch_size = tf.placeholder(tf.int32)\n",
    "    \n",
    "    decoder_in = tf.placeholder(tf.int32, [None, None])\n",
    "    decoder_lens = tf.placeholder(tf.int32, [None])\n",
    "    labels = tf.placeholder(tf.int32, [None, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the seq2seq, get the relevant tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"model\"):\n",
    "    logits, _ = basic_seq2seq(encoder_in, encoder_lens, decoder_in, decoder_lens,\n",
    "                                          vocab_size=vocab_size, batch_size=batch_size, lstm_type=\"gru\",\n",
    "                                          lstm_dim=lstm_dim, keep_prob=keep_prob, max_iterations=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss (crossent and L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    #labels_flat = tf.reshape(labels, [-1])\n",
    "    #logits = tf.reshape(logits, [-1, vocab_size])\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits)\n",
    "    \n",
    "    train_loss = (tf.reduce_sum(crossent) / tf.cast(batch_size, tf.float32))\n",
    "    \n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    reg_loss = l2reg * sum(reg_losses)\n",
    "    tv = tf.trainable_variables()\n",
    "    reg_losses.extend([l2reg * tf.nn.l2_loss(v) for v in tv])\n",
    "\n",
    "    loss = train_loss + reg_loss\n",
    "    \n",
    "    with tf.name_scope(\"logging\"):\n",
    "        tf.summary.scalar(\"train_loss\", train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer (Adam, gradient clipping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"optimizer\"):\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        gvs = optimizer.compute_gradients(loss)\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, 5.), var) for grad, var in gvs]\n",
    "        train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
    "    \n",
    "        with tf.name_scope(\"logging\"):\n",
    "            for grad, var in capped_gvs:\n",
    "                tf.summary.histogram(var.name + \"_grads\", grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish up with some logging hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"logging\"):\n",
    "    valid_loss_ph = tf.placeholder(tf.float32, name=\"validation_loss\")\n",
    "    \n",
    "    tf.summary.scalar(\"Valid_loss\", valid_loss_ph)\n",
    "\n",
    "    #Add histograms for trainable variables\n",
    "    for v in tf.trainable_variables():\n",
    "        tf.summary.histogram(v.name, v)    \n",
    "\n",
    "    log_op = tf.summary.merge_all()\n",
    "\n",
    "writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop. On each logging interval, we calculate the validation loss and save the weights when the loss improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(traindb_file)\n",
    "encoder_in_batch = data['enc_in']\n",
    "encoder_len_batch = data['enc_lens']\n",
    "decoder_in_batch = data['dec_in']\n",
    "decoder_len_batch = data['dec_lens']\n",
    "labels_batch = data['labels']\n",
    "\n",
    "valid_data = np.load(testdb_file)\n",
    "valid_encoder_in_batch = data['enc_in']\n",
    "valid_encoder_len_batch = data['enc_lens']\n",
    "valid_decoder_in_batch = data['dec_in']\n",
    "valid_decoder_len_batch = data['dec_lens']\n",
    "valid_labels_batch = data['labels']\n",
    "\n",
    "n_examples = labels_batch.shape[0]\n",
    "idx = np.arange(n_examples)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "i = 0\n",
    "k = 0\n",
    "min_loss_val = 10000.0\n",
    "while i < n_epochs:\n",
    "    print(\"EPOCH %d\"%i)\n",
    "    j = 0\n",
    "    np.random.shuffle(idx)\n",
    "    while j < n_examples:\n",
    "        curr = idx[j:j+batch_size_val]\n",
    "        batch_size_curr = len(curr)\n",
    "        if k % log_interval == 0:\n",
    "            #Get the validation loss\n",
    "            valid_loss_val, logits_val = sess.run([loss, logits], feed_dict={encoder_in: valid_encoder_in_batch,\n",
    "                                                       encoder_lens: valid_encoder_len_batch,\n",
    "                                                       decoder_in: valid_decoder_in_batch,\n",
    "                                                       decoder_lens: valid_decoder_len_batch,\n",
    "                                                       labels: valid_labels_batch,\n",
    "                                                       batch_size: valid_labels_batch.shape[0]})\n",
    "            \n",
    "            \n",
    "            summary, _, loss_val, logits_val = sess.run([log_op, train_op, loss, logits], feed_dict={encoder_in: encoder_in_batch[curr],\n",
    "                                                       encoder_lens: encoder_len_batch[curr],\n",
    "                                                       decoder_in: decoder_in_batch[curr],\n",
    "                                                       decoder_lens: decoder_len_batch[curr],\n",
    "                                                       labels: labels_batch[curr],\n",
    "                                                       batch_size: batch_size_curr,\n",
    "                                                       valid_loss_ph: valid_loss_val})\n",
    "            writer.add_summary(summary, k)\n",
    "            \n",
    "            if valid_loss_val < min_loss_val:\n",
    "                print(valid_loss_val)\n",
    "                min_loss_val = valid_loss_val\n",
    "                saver.save(sess, checkpoint_file % k)\n",
    "        else:\n",
    "            _, loss_val, logits_val = sess.run([train_op, loss, logits], feed_dict={encoder_in: encoder_in_batch[curr],\n",
    "                                                       encoder_lens: encoder_len_batch[curr],\n",
    "                                                       decoder_in: decoder_in_batch[curr],\n",
    "                                                       decoder_lens: decoder_len_batch[curr],\n",
    "                                                       labels: labels_batch[curr],\n",
    "                                                       batch_size: batch_size_curr})\n",
    "        \n",
    "        \n",
    "        j += batch_size_val\n",
    "        k += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
