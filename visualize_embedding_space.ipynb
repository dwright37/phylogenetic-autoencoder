{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the embedding space\n",
    "\n",
    "Contains code to generate embeddings for the test data as well as plot and save to a checkpoint file which can be plotting using tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from model.seq2seq import bilstm, lstm, gru\n",
    "import numpy as np\n",
    "from utils.text_processing import load_dict_from_vocab_file\n",
    "\n",
    "vocab_file = './data/character_inventory_unk.txt'\n",
    "traindb_file = './data/training.npz'\n",
    "testdb_file = './data/testing.npz'\n",
    "checkpoint_file = './tfmodel/gru_enc/model_800.tfmodel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = load_dict_from_vocab_file(vocab_file)\n",
    "vocab_size = len(vocab)\n",
    "lstm_dim = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"placeholders\"):\n",
    "    encoder_in = tf.placeholder(tf.int32, [None, None])\n",
    "    encoder_lens = tf.placeholder(tf.int32, [None])\n",
    "    batch_size = tf.placeholder(tf.int32)\n",
    "\n",
    "with tf.name_scope(\"model\"):\n",
    "    enc_emb = tf.one_hot(encoder_in, vocab_size)\n",
    "    with tf.name_scope(\"encoder\"):\n",
    "        encoder_state = gru(enc_emb, encoder_lens, lstm_dim, batch_size=batch_size, scope='gru')\n",
    "        \n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load(testdb_file)\n",
    "encoder_in_batch = data['enc_in']\n",
    "encoder_len_batch = data['enc_lens']\n",
    "\n",
    "embeddings = sess.run(encoder_state, feed_dict={encoder_in: encoder_in_batch, \n",
    "                                                   encoder_lens: encoder_len_batch,\n",
    "                                                   batch_size: encoder_in_batch.shape[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import mpld3\n",
    "\n",
    "%matplotlib notebook\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "def scatterPlotWithLabels(ax,x,y,z=[], pointSize=5,labels=None,color='red'):\n",
    "    if len(z) > 0:\n",
    "        handle = ax.scatter(x, y, z, c=color, s=pointSize)\n",
    "    else:\n",
    "        handle = ax.scatter(x, y, c=color, s=pointSize)\n",
    "    \n",
    "    if labels is not None:\n",
    "        tooltip = mpld3.plugins.PointLabelTooltip(handle, labels=labels)\n",
    "        mpld3.plugins.connect(plt.gcf(), tooltip)\n",
    "    return handle\n",
    "\n",
    "def get_cmap(n, name='RdYlBu'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "def plotSamples(classes, xform, classnames, projection=None, title=None): \n",
    "    colormap = get_cmap(len(classes))\n",
    "    fig = plt.figure(figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    ax = plt.subplot(facecolor='#EEEEEE', projection=projection)\n",
    "    #ax.grid(color='white', linestyle='solid')\n",
    "    if title is not None:\n",
    "        ax.set_title(title, fontsize=30)\n",
    "    handles = []\n",
    "    labels = []\n",
    "    point_labels = []\n",
    "    for i,c in enumerate(classes):\n",
    "        plot_data = xform.transform(c)\n",
    "        if projection and projection in '3d':\n",
    "            ex = scatterPlotWithLabels(ax, plot_data[:,0], plot_data[:,1], plot_data[:,2], color=colormap(i))\n",
    "        else:\n",
    "            ex = scatterPlotWithLabels(ax, plot_data[:,0], plot_data[:,1], color=colormap(i))\n",
    "\n",
    "        handles.append(ex)\n",
    "        labels.append(classnames[i])\n",
    "\n",
    "#     handles, labels = ax.get_legend_handles_labels() # return lines and labels\n",
    "#     interactive_legend = mpld3.plugins.InteractiveLegendPlugin(zip(handles,\n",
    "#                                                              ax.collections),\n",
    "#                                                          labels,\n",
    "#                                                          #alpha_unsel=0.5,\n",
    "#                                                          #alpha_over=1.5, \n",
    "#                                                          start_visible=True)\n",
    "#     #ax.legend(handles, labels, markerscale=0.5)\n",
    "#     mpld3.plugins.connect(fig, interactive_legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/testing.txt', sep='\\t')\n",
    "classes = [d.split(';')[0] for d in data.values[:,3]]\n",
    "embed_pca = PCA(n_components=3)\n",
    "embed_decomp = embed_pca.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "classnames = [j for j,_ in groupby(classes)]\n",
    "count_dups = [sum(1 for _ in group) for _, group in groupby(classes)]\n",
    "embeddings_class_split = np.split(embeddings, np.cumsum(count_dups))[:-1]\n",
    "\n",
    "plotSamples(embeddings_class_split, embed_decomp, classnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save embeddings as model checkpoint in order to plot in tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import os\n",
    "\n",
    "embedding_var = tf.Variable(embeddings, name='phylogenetic_tree')\n",
    "\n",
    "# Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "config = projector.ProjectorConfig()\n",
    "\n",
    "# You can add multiple embeddings. Here we add only one.\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "# Link this tensor to its metadata file (e.g. labels).\n",
    "embedding.metadata_path = 'testing.tsv'\n",
    "\n",
    "# Use the same LOG_DIR where you stored your checkpoint.\n",
    "summary_writer = tf.summary.FileWriter('./tb/embeddings')\n",
    "\n",
    "# The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n",
    "# read this file during startup.\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "sess2 = tf.Session()\n",
    "sess2.run(tf.global_variables_initializer())\n",
    "saver2 = tf.train.Saver()\n",
    "saver2.save(sess2, './tb/embeddings/model.ckpt', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
